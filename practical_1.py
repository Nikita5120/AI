# -*- coding: utf-8 -*-
"""Practical_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zj2-cF3piW9qTGUO1rzf4XThmcabRCIj
"""

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ML models
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target
class_names = data.target_names

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features for SVM
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM": SVC(kernel='linear', probability=True)
}

results = {}
conf_matrices = {}

# Train and evaluate each model
for name, model in models.items():
    if name == "SVM":
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    conf_matrices[name] = confusion_matrix(y_test, y_pred)

    print(f"\n{name} Report:\n", classification_report(y_test, y_pred, target_names=class_names))

# Visualize Accuracy Comparison
plt.figure(figsize=(8,5))
sns.barplot(x=list(results.keys()), y=list(results.values()))
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison")
plt.ylim(0.8, 1.0)
plt.show()

# Visualize Confusion Matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for ax, (name, cm) in zip(axes, conf_matrices.items()):
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_title(f'{name} Confusion Matrix')
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score

# Store detailed metrics
metrics_df = pd.DataFrame(columns=["Model", "Accuracy", "Precision", "Recall", "F1", "ROC-AUC"])

# Plot ROC and PR curves
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.title("ROC Curve")

plt.subplot(1, 2, 2)
plt.title("Precision-Recall Curve")

for name, model in models.items():
    if name == "SVM":
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_score = model.decision_function(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_score = model.predict_proba(X_test)[:, 1]

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['precision']
    rec = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['recall']
    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']
    roc_auc = roc_auc_score(y_test, y_score)

    metrics_df = pd.concat([metrics_df, pd.DataFrame({
        "Model": [name],
        "Accuracy": [acc],
        "Precision": [prec],
        "Recall": [rec],
        "F1": [f1],
        "ROC-AUC": [roc_auc]
    })], ignore_index=True)

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_score)
    plt.subplot(1, 2, 1)
    plt.plot(fpr, tpr, label=name)

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_test, y_score)
    plt.subplot(1, 2, 2)
    plt.plot(recall, precision, label=name)

# Finalize ROC and PR curves
plt.subplot(1, 2, 1)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()

plt.subplot(1, 2, 2)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()

plt.tight_layout()
plt.show()

# ðŸ“Š Bar chart for all metrics
metrics_df.set_index("Model").plot(kind='bar', figsize=(10,6))
plt.title("Comparison of Models Based on Various Metrics")
plt.ylabel("Score")
plt.ylim(0.7, 1.0)
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.legend(loc='lower right')
plt.show()

# Show the table
print("ðŸ“‹ Detailed Metrics:")
print(metrics_df.round(3))

